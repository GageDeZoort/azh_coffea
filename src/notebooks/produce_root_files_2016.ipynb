{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from coffea import util\n",
    "import uproot\n",
    "import numpy as np\n",
    "#import mplhep as hep\n",
    "import hist\n",
    "from hist import Hist\n",
    "from matplotlib import pyplot as plt\n",
    "from cycler import cycler\n",
    "from hist.axis import Variable\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from azh_analysis.utils.histograms import integrate, norm_to\n",
    "#from azh_analysis.utils.plotting import plot_closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_hist():\n",
    "    return Hist(\n",
    "        Variable(\n",
    "            [\n",
    "                200,\n",
    "                220,\n",
    "                240,\n",
    "                260,\n",
    "                280,\n",
    "                300,\n",
    "                320,\n",
    "                340,\n",
    "                360,\n",
    "                380,\n",
    "                400,\n",
    "                450,\n",
    "                550,\n",
    "                700,\n",
    "                1000,\n",
    "                2400,\n",
    "            ],\n",
    "            name=\"mass\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "year = \"2016\"\n",
    "indir = \"../output_unblind\"\n",
    "\n",
    "mc_pre = util.load(f'{indir}/MC_UL_{year}preVFP_all_OS.coffea')\n",
    "mc_post = util.load(f'{indir}/MC_UL_{year}postVFP_all_OS.coffea')\n",
    "signal_pre = util.load(f'{indir}/signal_UL_{year}preVFP_all_OS.coffea')\n",
    "signal_post = util.load(f'{indir}/signal_UL_{year}postVFP_all_OS.coffea')\n",
    "data_pre = util.load(f'{indir}/data_UL_{year}preVFP_OS_ub.coffea')\n",
    "data_post = util.load(f'{indir}/data_UL_{year}postVFP_OS_ub.coffea')\n",
    "data_ss_pre = util.load(f'{indir}/data_UL_{year}preVFP_SS_ub_not-relaxed.coffea')\n",
    "data_ss_post = util.load(f'{indir}/data_UL_{year}postVFP_SS_ub_not-relaxed.coffea')\n",
    "data_ssr_pre = util.load(f\"{indir}/data_UL_{year}preVFP_SS_ub_relaxed.coffea\")\n",
    "data_ssr_post = util.load(f\"{indir}/data_UL_{year}postVFP_SS_ub_relaxed.coffea\")\n",
    "\n",
    "group_labels = { \n",
    "    \"2016postVFP\": {\n",
    "        \"TT\": [\n",
    "            \"TTToSemiLeptonic\",\n",
    "            \"TTToHadronic\",\n",
    "            \"TTTo2L2Nu\",\n",
    "        ],\n",
    "        \"TTZ\": [\n",
    "            \"ttZJets\",\n",
    "        ],    \n",
    "        \"TTW\": [\n",
    "            \"TTWJetsToLNu\",\n",
    "        ],\n",
    "        \"ggZZ\": [\n",
    "            \"GluGluToContinToZZTo2e2tau\",\n",
    "            \"GluGluToContinToZZTo2mu2tau\",\n",
    "            \"GluGluToContinToZZTo4e\",\n",
    "            \"GluGluToContinToZZTo4mu\",\n",
    "            \"GluGluToContinToZZTo4tau\",\n",
    "        ],\n",
    "        \"ZZ\": [\n",
    "            \"ZZTo4L\",\n",
    "            \"ZZTo2Q2Lmllmin4p0\",\n",
    "        ],\n",
    "        \"WZ\": [\n",
    "            \"WZTo2Q2L\",\n",
    "            \"WZTo3LNu\",\n",
    "        ],\n",
    "        \"VVV\": [\n",
    "            \"WWW4F\",\n",
    "            \"WWW4F_ext1\",\n",
    "            \"WWZ4F\",\n",
    "            \"WZZ_ext1\",\n",
    "            \"ZZZ\",\n",
    "        ],\n",
    "        \"ggHtt\": [\n",
    "            \"GluGluHToTauTauM125\",\n",
    "        ],\n",
    "        \"VBFHtt\": [\n",
    "            \"VBFHToTauTauM125\",\n",
    "        ],\n",
    "        \"WHtt\": [\n",
    "            \"WminusHToTauTauM125\",\n",
    "            \"WplusHToTauTauM125\",\n",
    "        ],\n",
    "        \"ZHtt\": [\n",
    "            \"ZHToTauTauM125_ext1\",\n",
    "        ],\n",
    "        \"TTHtt\": [\n",
    "            \"ttHToTauTauM125\",\n",
    "        ],\n",
    "        \"ggHWW\": [\n",
    "            \"GluGluHToWWTo2L2NuM-125\",\n",
    "        ],\n",
    "        \"VBFHWW\": [\n",
    "            \"VBFHToWWTo2L2NuM-125\",\n",
    "        ], \n",
    "        \"ggZHWW\": [\n",
    "            \"GluGluZHHToWW\",\n",
    "        ],\n",
    "        \"ggHZZ\": [\n",
    "            \"GluGluHToZZTo4LM125\",\n",
    "        ],\n",
    "        \"WHWW\": [\n",
    "            \"HWminusJHToWW\",\n",
    "            \"HWplusJHToWW\",\n",
    "        ],\n",
    "        \"ZHWW\": [\n",
    "            \"HZJHToWW\",\n",
    "            \"HZJHToWW_ext1\"\n",
    "        ]\n",
    "    },\n",
    "    \"2016preVFP\": {\n",
    "        \"TT\": [\n",
    "            \"TTToSemiLeptonic\",\n",
    "            \"TTToHadronic\",\n",
    "            \"TTTo2L2Nu\",\n",
    "        ],\n",
    "        \"TTZ\": [\n",
    "            \"ttZJets\",\n",
    "        ],    \n",
    "        \"TTW\": [\n",
    "            \"TTWJetsToLNu\",\n",
    "        ],\n",
    "        \"ggZZ\": [\n",
    "            \"GluGluToContinToZZTo2e2tau\",\n",
    "            \"GluGluToContinToZZTo2mu2tau\",\n",
    "            \"GluGluToContinToZZTo4e\",\n",
    "            \"GluGluToContinToZZTo4mu\",\n",
    "            \"GluGluToContinToZZTo4tau\",\n",
    "        ],\n",
    "        \"ZZ\": [\n",
    "            \"ZZTo4L\",\n",
    "            \"ZZTo2Q2L\",\n",
    "        ],\n",
    "        \"WZ\": [\n",
    "            \"WZTo2Q2Lmllmin4p0\",\n",
    "            \"WZTo3LNu\",\n",
    "        ],\n",
    "        \"VVV\": [\n",
    "            \"WWW4F\",\n",
    "            \"WWW4F_ext1\",\n",
    "            \"WWZ4F\",\n",
    "            #\"WZZTuneCP5\",\n",
    "            \"WZZ_ext1\",\n",
    "            \"ZZZ\",\n",
    "            #\"ZZZTuneCP5_ext1\",\n",
    "        ],\n",
    "        \"ggHtt\": [\n",
    "            \"GluGluHToTauTauM125\",\n",
    "        ],\n",
    "        \"VBFHtt\": [\n",
    "            \"VBFHToTauTauM125\",\n",
    "        ],\n",
    "        \"WHtt\": [\n",
    "            \"WminusHToTauTauM125\",\n",
    "            \"WplusHToTauTauM125\",\n",
    "        ],\n",
    "        \"ZHtt\": [\n",
    "            \"ZHToTauTau_ext1\",\n",
    "        ],\n",
    "        \"TTHtt\": [\n",
    "            \"ttHToTauTauM125\",\n",
    "        ],\n",
    "        \"ggHWW\": [\n",
    "            \"GluGluHToWWTo2L2NuM125\",\n",
    "        ],\n",
    "        \"VBFHWW\": [\n",
    "            \"VBFHToWWTo2L2NuM-125\",\n",
    "        ], \n",
    "        \"ggZHWW\": [\n",
    "            \"GluGluZHHToWW\",\n",
    "        ],\n",
    "        \"ggHZZ\": [\n",
    "            \"GluGluHToZZTo4LM125\",\n",
    "        ],\n",
    "        \"WHWW\": [\n",
    "            \"HWminusJHToWW\",\n",
    "            \"HWplusJHToWW\",\n",
    "        ],\n",
    "        \"ZHWW\": [\n",
    "            \"HZJHToWW\",\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "cats = [\"eeet\", \"eemt\", \"eett\", \"eeem\", \"mmet\", \"mmmt\", \"mmtt\", \"mmem\"]\n",
    "systs = [\n",
    "    \"nom\",\n",
    "    \"l1prefire_up\",\n",
    "    \"l1prefire_down\",\n",
    "    \"pileup_up\",\n",
    "    \"pileup_down\",\n",
    "    \"tauES_down\",\n",
    "    \"tauES_up\",\n",
    "    \"efake_down\",\n",
    "    \"efake_up\",\n",
    "    \"mfake_down\",\n",
    "    \"mfake_up\",\n",
    "    \"eleES_down\",\n",
    "    \"eleES_up\",\n",
    "    \"eleSmear_down\",\n",
    "    \"eleSmear_up\",\n",
    "    \"muES_down\",\n",
    "    \"muES_up\",\n",
    "    \"unclMET_down\",\n",
    "    \"unclMET_up\",\n",
    "    \"tauID_0_down\",\n",
    "    \"tauID_0_up\",\n",
    "    \"tauID_1_down\",\n",
    "    \"tauID_1_up\",\n",
    "    \"tauID_10_down\",\n",
    "    \"tauID_10_up\",\n",
    "    \"tauID_11_down\",\n",
    "    \"tauID_11_up\",\n",
    "    \"JES_up\",\n",
    "    \"JES_down\",\n",
    "    \"JER_up\",\n",
    "    \"JER_down\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1cc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"m4l_binopt\"\n",
    "outdir = \"for_alexis/towards_unblinding/datacard_templates\"\n",
    "for b in [0, 1]:\n",
    "    btag_label = \"btag\" if (b==1) else \"0btag\"\n",
    "    print(btag_label)\n",
    "    \n",
    "    ############################\n",
    "    # fill MC output ROOT file #\n",
    "    ############################\n",
    "    mc_file = uproot.recreate(f\"{outdir}/MC_{btag_label}_{year}.root\")\n",
    "    for group, _ in group_labels[\"2016postVFP\"].items():\n",
    "        \n",
    "        \n",
    "        # grab MC preVFP group\n",
    "        datasets_pre = group_labels[\"2016preVFP\"][group]\n",
    "         \n",
    "        factors_pre = {group_dataset: 1.0 for group_dataset in datasets_pre}\n",
    "        #if \"ggZZ\" in group and \"output_unblind\" in indir:\n",
    "        #    factors_pre = {\n",
    "        #        group_dataset: 0.0027/0.00159 if \"4\" in group_dataset else 0.0054/0.00319\n",
    "        #        for group_dataset in datasets_pre\n",
    "        #    }\n",
    "        #print(group, datasets_pre, factors_pre)\n",
    "        \n",
    "        mc_group_pre = sum(factors_pre[k]*v for k, v in mc_pre[var].items() if k in datasets_pre)\n",
    "        found = [k for k, _ in mc_pre[var].items() if k in datasets_pre]\n",
    "        if len(found)!=len(datasets_pre):\n",
    "            print(f\"ERROR: (preVFP) found {found} expected {datasets_pre}\")\n",
    "            \n",
    "            \n",
    "        # grab MC postVFP group\n",
    "        datasets_post = group_labels[\"2016postVFP\"][group]\n",
    "        \n",
    "        factors_post = {group_dataset: 1.0 for group_dataset in datasets_post}\n",
    "        #if \"ggZZ\" in group and \"output_unblind\" in indir:\n",
    "        #    factors_post = {\n",
    "        #        group_dataset: 0.0027/0.00159 if \"4\" in group_dataset else 0.0054/0.00319\n",
    "        #        for group_dataset in datasets_post\n",
    "        #    }\n",
    "        #print(group, datasets_post, factors_post)\n",
    "        \n",
    "        mc_group_post = sum(factors_post[k]*v for k, v in mc_post[var].items() if k in datasets_post)\n",
    "        found = [k for k, _ in mc_post[var].items() if k in datasets_post]\n",
    "        if len(found)!=len(datasets_post):\n",
    "            print(f\"ERROR: (postVFP) found {found} expected {datasets_post}\")\n",
    "            \n",
    "        # combine post and pre VFP\n",
    "        mc_group = mc_group_pre + mc_group_post\n",
    "        \n",
    "        for cat in cats:\n",
    "            for syst in systs:         \n",
    "                if (\"btag\" in syst): continue\n",
    "                if (\n",
    "                    (cat not in list(mc_group.axes[1])) or\n",
    "                    (syst not in list(mc_group.axes[3]))\n",
    "                   ):\n",
    "                    group_hist = get_empty_hist()\n",
    "                else: \n",
    "                    group_hist = mc_group[::sum, cat, b, syst, \"cons\", :]       \n",
    "                \n",
    "                if \"tauID\" in syst:\n",
    "                    syst = \"tauID\" + syst.split(\"_\")[1] + \"_\" + syst.split(\"_\")[2]\n",
    "                \n",
    "                if \"nom\" in syst:\n",
    "                    fname = f\"{cat}/{group}\"\n",
    "                    mc_file[fname] =  group_hist\n",
    "                else:\n",
    "                    shift = syst.split(\"_\")[-1]\n",
    "                    syst = syst.replace(f\"_{shift}\", \"\")\n",
    "                    syst = syst + shift.capitalize()\n",
    "                    fname = f\"{cat}/{group}_{syst}\"\n",
    "                    mc_file[fname] = group_hist\n",
    "                    \n",
    "            # extract the statistical bin errors\n",
    "            #variances = defaultdict(list)\n",
    "            #for k, h in mc_pre[\"m4l\"].items():\n",
    "            #    if k not in datasets_pre: continue\n",
    "            #    if cat not in list(h.axes[1]): continue\n",
    "            #    h = h[::sum, cat, b, \"nom\", \"cons\", :]\n",
    "            #    var = np.array(h.variances())\n",
    "            #    for idx, _ in enumerate(np.array(h.axes[0])):\n",
    "            #        variances[str(idx)].append(var[idx])\n",
    "            #        #print(\"pre\", k, cat, idx, var[idx])      \n",
    "            #for k, h in mc_post[\"m4l\"].items():\n",
    "            #    if k not in datasets_post: continue\n",
    "            #    if cat not in list(h.axes[1]): continue\n",
    "            #    h = h[::sum, cat, b, \"nom\", \"cons\", :]\n",
    "            #    var = np.array(h.variances())\n",
    "            #    for idx, _ in enumerate(np.array(h.axes[0])):\n",
    "            #        variances[str(idx)].append(var[idx])\n",
    "            #        #print(\"post\", k, cat, idx, var[idx])\n",
    "            \n",
    "            # attempt to fill group\n",
    "            #if len(variances)==0: \n",
    "            #    print(\"skipping\", group)\n",
    "            #    continue\n",
    "            #group_hist = mc_group[::sum, cat, b, \"nom\", \"cons\", :]\n",
    "            #values = group_hist.values()\n",
    "            #stds = {k: np.sqrt(sum(v)) for k, v in variances.items()}\n",
    "            #for idx in range(15):\n",
    "            #    sidx = str(idx)\n",
    "            #    if sidx not in list(variances.keys()): \n",
    "            #        print(\"skipping\", group, cat, idx)\n",
    "            #        continue\n",
    "            #    if stds[sidx] <= 0 or values[idx] <= 0.4: \n",
    "            #        print(\"skipping\", group, cat, idx)\n",
    "            #        continue\n",
    "            #    up_name = f\"{cat}/{group}_{group}-{cat}-bin{idx}Up\"\n",
    "            #    up_hist = group_hist.copy()\n",
    "            #    up_hist[idx] = (values[idx] + stds[sidx], 0.0)\n",
    "            #    #print(idx, \"up\", up_hist)\n",
    "            #    mc_file[up_name] = up_hist\n",
    "            #    down_name = f\"{cat}/{group}_{group}-{cat}-bin{idx}Down\"\n",
    "            #    down_hist = group_hist.copy()\n",
    "            #    #print(idx, \"down\", down_hist)\n",
    "            #    down_hist[idx] = (max(10**-9, values[idx] - stds[sidx]), 0.0)\n",
    "            #    mc_file[down_name] = down_hist\n",
    "            #    print(cat, idx, values[idx] - stds[sidx], values[idx], values[idx] + stds[sidx])\n",
    "                \n",
    "  \n",
    "    ###################################\n",
    "    # fill reducible into the MC file #\n",
    "    ###################################\n",
    "    data_group_pre = sum(v for k, v in data_pre[var].items()) \n",
    "    data_reg_group_pre = sum(v for k, v in data_pre[\"m4l_reg\"].items())\n",
    "    data_group_post = sum(v for k, v in data_post[var].items())\n",
    "    data_reg_group_post = sum(v for k, v in data_post[\"m4l_reg\"].items())\n",
    "    data_group = data_group_pre + data_group_post\n",
    "    data_reg_group = data_reg_group_pre + data_reg_group_post\n",
    "    \n",
    "    data_ss_group_pre = sum(v for k, v in data_ss_pre[var].items())\n",
    "    data_ss_reg_group_pre = sum(v for k, v in data_ss_pre[\"m4l_reg\"].items())\n",
    "    data_ss_group_post = sum(v for k, v in data_ss_post[var].items())\n",
    "    data_ss_reg_group_post = sum(v for k, v in data_ss_post[\"m4l_reg\"].items())\n",
    "    data_ss_group = data_ss_group_pre + data_ss_group_post\n",
    "    data_ss_reg_group = data_ss_reg_group_pre + data_ss_reg_group_post\n",
    "    \n",
    "    data_ssr_group_pre = sum(v for k, v in data_ssr_pre[var].items())\n",
    "    data_ssr_reg_group_pre = sum(v for k, v in data_ssr_pre[\"m4l_reg\"].items())\n",
    "    data_ssr_group_post = sum(v for k, v in data_ssr_post[var].items())\n",
    "    data_ssr_reg_group_post = sum(v for k, v in data_ssr_post[\"m4l_reg\"].items())\n",
    "    data_ssr_group = data_ssr_group_pre + data_ssr_group_post\n",
    "    data_ssr_reg_group = data_ssr_reg_group_pre + data_ssr_reg_group_post\n",
    "        \n",
    "    for cat in cats:\n",
    "        group_hist = data_group[\"reducible\", cat, b, ::sum, \"cons\", :]\n",
    "        group_ss_hist = data_ss_group[\"reducible\", cat, b, ::sum, \"cons\", :] \n",
    "        group_ssr_hist = data_ssr_group[\"data\", cat, b, ::sum, \"cons\", :]\n",
    "        group_reg_hist = data_reg_group[\"reducible\", cat, b, ::sum, \"cons\", :]\n",
    "        group_ss_reg_hist = data_ss_reg_group[\"reducible\", cat, b, ::sum, \"cons\", :] \n",
    "        group_ssr_reg_hist = data_ssr_reg_group[\"data\", cat, b, ::sum, \"cons\", :]\n",
    "        \n",
    "        # norm the m4l\n",
    "        group_ssr_hist = norm_to(group_hist, group_ssr_hist, simple=False)\n",
    "        group_ss_hist = norm_to(group_hist, group_ss_hist, simple=False)\n",
    "        \n",
    "        # m4l_reg\n",
    "        group_ssr_reg_hist = norm_to(group_reg_hist, group_ssr_reg_hist, simple=False)\n",
    "        group_ss_reg_hist = norm_to(group_reg_hist, group_ss_reg_hist, simple=False)\n",
    "        \n",
    "        for i, ax in enumerate(group_ssr_reg_hist.axes[0]):\n",
    "            group_ssr_hist_up = group_ssr_hist.copy()\n",
    "            group_ssr_hist_down = group_ssr_hist.copy()\n",
    "            if i>4 or i==0: continue\n",
    "            down, up = ax\n",
    "            ssr_val, ssr_std = group_ssr_reg_hist[i].value, np.sqrt(group_ssr_reg_hist[i].variance)\n",
    "            ss_val, ss_std = group_ss_reg_hist[i].value, np.sqrt(group_ss_reg_hist[i].variance)\n",
    "            sigma = ss_std / ssr_val if ssr_val > 0 else 0\n",
    "            print(i, ax, sigma)\n",
    "            for j, bx in enumerate(group_ssr_hist.axes[0]):\n",
    "                bdown, bup = bx\n",
    "                #if bup <= up and bup >= down and bdown <= up and bdown >= down:\n",
    "                if (\n",
    "                    (i==1 and j in [0,1,2,3,4,5,6,7,8,9]) or\n",
    "                    (i==2 and j in [10, 11]) or \n",
    "                    (i==3 and j in [12, 13]) or\n",
    "                    (i==4 and j in [14]) \n",
    "                ):\n",
    "                    group_ssr_hist_up[j] = (group_ssr_hist_up[j].value * (1 + sigma), group_ssr_hist_up[j].variance)\n",
    "                    group_ssr_hist_down[j] = (group_ssr_hist_down[j].value * 1/(1+sigma), group_ssr_hist_down[j].variance) \n",
    "                    \n",
    "            fnameUp = f\"{cat}/reducible_closure{i}Up\"\n",
    "            mc_file[fnameUp] = group_ssr_hist_up\n",
    "            fnameDown = f\"{cat}/reducible_closure{i}Down\"\n",
    "            mc_file[fnameDown] = group_ssr_hist_down\n",
    "                    \n",
    "        fname = f\"{cat}/reducible\"\n",
    "        mc_file[fname] = group_ssr_hist\n",
    "        \n",
    "       \n",
    "    ##########################    \n",
    "    # now fill the fill data #\n",
    "    ##########################\n",
    "    data_file = uproot.recreate(f\"{outdir}/data_{btag_label}_{year}.root\")\n",
    "    for cat in cats:\n",
    "        if (cat not in list(data_group.axes[1])):\n",
    "            group_hist = get_empty_hist()\n",
    "        else: \n",
    "            group_hist = data_group[\"data\", cat, b, ::sum, \"cons\", :]\n",
    "\n",
    "        fname = f\"{cat}/data\"\n",
    "        data_file[fname] = group_hist\n",
    "    \n",
    "    #######################\n",
    "    # now fill for signal #\n",
    "    #######################\n",
    "    unique_masses = np.unique([f\"{k.split('TauM')[-1]}\" for k in signal_pre[var].keys()])\n",
    "    \n",
    "    ggA_masses_pre = {\n",
    "        m: [v for k, v in signal_pre[var].items() \n",
    "            if k.split(\"TauM\")[-1]==m and \"Glu\" in k]\n",
    "        for m in unique_masses\n",
    "    }\n",
    "    ggA_masses_pre = {k: v[0] if len(v)>0 else None for k, v in ggA_masses_pre.items()}\n",
    "    \n",
    "    ggA_masses_post = {\n",
    "        m: [v for k, v in signal_post[var].items() \n",
    "            if k.split(\"TauM\")[-1]==m and \"Glu\" in k]\n",
    "        for m in unique_masses\n",
    "    }\n",
    "    ggA_masses_post = {k: v[0] if len(v)>0 else None for k, v in ggA_masses_post.items()}\n",
    "     \n",
    "    bbA_masses_pre = {\n",
    "        m: [v for k, v in signal_pre[var].items() \n",
    "            if k.split(\"TauM\")[-1]==m and \"BB\" in k]\n",
    "        for m in unique_masses\n",
    "    }\n",
    "    bbA_masses_pre = {k: v[0] if len(v)>0 else None for k, v in bbA_masses_pre.items()}\n",
    "\n",
    "    bbA_masses_post = {\n",
    "        m: [v for k, v in signal_post[var].items() \n",
    "            if k.split(\"TauM\")[-1]==m and \"BB\" in k]\n",
    "        for m in unique_masses\n",
    "    }\n",
    "    bbA_masses_post = {k: v[0] if len(v)>0 else None for k, v in bbA_masses_post.items()}\n",
    "    \n",
    "    for m in unique_masses:\n",
    "        fname = f\"{outdir}/signal_{m}_{btag_label}_{year}.root\"\n",
    "        file = uproot.recreate(fname)\n",
    "        ggA = ggA_masses_pre[m] + ggA_masses_post[m]\n",
    "        bbA = bbA_masses_pre[m] + bbA_masses_post[m]\n",
    "        for k, v in {\"ggA\": ggA, \"bbA\": bbA}.items():\n",
    "            if v is None: \n",
    "                print(\"WARNING: SKIPPING\", k, m)\n",
    "                continue\n",
    "            for cat in cats:\n",
    "                for syst in systs:        \n",
    "                    if (\"btag\" in syst): continue\n",
    "                    if (\n",
    "                        (cat not in list(v.axes[1])) or\n",
    "                        (syst not in list(v.axes[3])) \n",
    "                       ):\n",
    "                        group_hist = get_empty_hist()\n",
    "                    else:\n",
    "                        group_hist = v[::sum, cat, b, syst, \"cons\", :] \n",
    "\n",
    "                    if \"tauID\" in syst:\n",
    "                        syst = \"tauID\" + syst.split(\"_\")[1] + \"_\" + syst.split(\"_\")[2]\n",
    "\n",
    "                    if \"nom\" in syst:\n",
    "                        fname = f\"{cat}/{k}\"\n",
    "                        file[fname] =  group_hist\n",
    "                    else:\n",
    "                        shift = syst.split(\"_\")[-1]\n",
    "                        syst = syst.replace(f\"_{shift}\", \"\")\n",
    "                        syst = syst + shift.capitalize()\n",
    "                        fname = f\"{cat}/{k}_{syst}\"\n",
    "                        file[fname] = group_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd20e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate(hist):\n",
    "    bins = np.array(hist.axes[0])\n",
    "    widths = bins[:,1] - bins[:,0]\n",
    "    vals = hist.values()\n",
    "    return sum(widths*vals)\n",
    "\n",
    "for b in [0, 1]:\n",
    "    btag_label = \"btag\" if (b==1) else \"0btag\"\n",
    "    data_group = sum(\n",
    "        v for k, v in data[\"m4l\"].items()\n",
    "    )   \n",
    "    data_ss_group = sum(\n",
    "        v for k, v in data_ss[\"m4l\"].items()\n",
    "    )\n",
    "    \n",
    "    # fill reducible into the MC file\n",
    "    for cat in cats:\n",
    "        if (cat not in list(data_group.axes[1])):\n",
    "            group_hist = get_empty_hist()\n",
    "        else: \n",
    "            group_hist = data_group[\"reducible\", cat, b, ::sum, \"cons\", :]\n",
    "            group_ss_hist = data_ss_group[::sum, cat, b, ::sum, \"cons\", :] \n",
    "            os_norm = integrate(group_hist)\n",
    "            ss_norm = integrate(group_ss_hist)\n",
    "            group_ss_hist = group_ss_hist * os_norm / ss_norm\n",
    "            group_hist.plot1d(histtype=\"step\")\n",
    "            group_ss_hist.plot1d(histtype=\"step\")\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uproot.open(\n",
    "    \"/eos/uscms/store/group/lpcsusyhiggs/ntuples/AZh/nAODv9/2017/BBAToZhToLLTauTauM1400/all_BBAToZhToLLTauTau_M1400_file001_part_1of3_Electrons.root\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a161cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "uproot.open(\n",
    "    '~/nobackup/combine/CMSSW_11_3_4/src/HiggsAnalysis/CombinedLimit/data/tutorials/longexercise/datacard_part3.shapes.root'\n",
    ").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "base = \"/eos/uscms/store/group/lpcsusyhiggs/ntuples/AZh/nAODv9/2017/BBAToZhToLLTauTauM1400/all_BBAToZhToLLTauTau_M1400_file001_part_1of3_Electrons.root\"\n",
    "uproot.open(base)[\"Events\"].arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "mass_points = [\n",
    "    '225','250','275','300','325','350','375','400','450',\n",
    "    '500','600','700','750','800','900','1000',\n",
    "    '1200','1400','1600','1800','2000'\n",
    "]\n",
    "for mass in mass_points:\n",
    "    for btag in ['0btag', 'btag']:\n",
    "        dir_1 = uproot.open(f\"root_for_combine/signal_{mass}_{btag}_2016postVFP.root\")\n",
    "        dir_2 = uproot.open(f\"root_for_combine/signal_{mass}_{btag}_2016preVFP.root\")\n",
    "        for j in dir_1.keys():\n",
    "            print(tree)\n",
    "            for k in dir_1[j].keys():\n",
    "                print(k)\n",
    "                h1 = dir_1[tree][k]\n",
    "                h2 = dir_2[tree][k]\n",
    "                print(h1.to_hist() + h2.to_hist())\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b4cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
